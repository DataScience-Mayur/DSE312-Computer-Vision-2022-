{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTbHHsc_Wiqs",
        "outputId": "b0328e4d-bd46-4b75-b28f-dec97dbb4caf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-contrib-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyFftyPMXTCf"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGCcQLA8Wx1W"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0V0BKRGW2fI"
      },
      "outputs": [],
      "source": [
        "original_image = cv2.imread(\"/content/touching_grayscale.png\")\n",
        "cv2_imshow(original_image)\n",
        "# print(original_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpF77-RGW4B6"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('/content/touching_grayscale.png', cv2.IMREAD_GRAYSCALE)\n",
        "cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCSi4aw8W6lm"
      },
      "outputs": [],
      "source": [
        "pixels_vals = original_image.reshape((-1, 3))\n",
        "pixel_vals = np.float32(pixels_vals)\n",
        "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "K = 10 #check at K = 3 and K = 5\n",
        "attempts = 10\n",
        "ret, label, center = cv2.kmeans(pixel_vals, K, None, criteria, attempts, cv2.KMEANS_PP_CENTERS)\n",
        "center = np.uint8(center)\n",
        "res = center[label.flatten()]\n",
        "result_image = res.reshape((original_image.shape))\n",
        "figure_size=15\n",
        "plt.figure(figsize=(figure_size, figure_size))\n",
        "plt.subplot(1,2,1), plt.imshow(original_image)\n",
        "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
        "plt.subplot(1,2,2), plt.imshow(result_image)\n",
        "plt.title('Segmented Image when K = %i' %K), plt.xticks([]), plt.yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngK9sDLkXDFp"
      },
      "outputs": [],
      "source": [
        "resultimage1 = cv2.cvtColor(result_image, cv2.COLOR_BGR2GRAY)\n",
        "otsu_threshold, image_result = cv2.threshold(resultimage1, 200, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "print(\"Obtained threshold: \", otsu_threshold)\n",
        "contours, hierarchy = cv2.findContours(image_result, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "for c in contours:\n",
        "  M = cv2.moments(c)\n",
        "  if M[\"m00\"] != 0:\n",
        "    cX = int(M[\"m10\"] / M[\"m00\"])\n",
        "    cY = int(M[\"m01\"] / M[\"m00\"])\n",
        "  else:\n",
        "    cX, cY = 0, 0\n",
        "  cv2.circle(resultimage1, (cX, cY), 1, (255, 0, 0), -1)\n",
        "cv2_imshow(resultimage1)\n",
        "cv2.drawContours(image_result, contours, -1, (0, 255, 0), 3)\n",
        "cv2_imshow(image_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLvArVbnXP6C"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy7Vnf3vXnWO"
      },
      "source": [
        "1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIkOlphlXPpY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "6926d645-751c-414d-9809-28bcf59c84d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimage_shape=size(image) \\nimages= Array [ ] \\nfor size in 1:5 \\nresized_image=image.reshape(image_shape//size) \\ninsert resized_image into images \\nend\\nblurred_images=[ ] \\nsigmas=Array[] # predecided values of sigma for example from 0.1,1.5,3.0,4.5,6.0 \\nfor i in 1:5 \\nscale=Array [] \\nfor sigma in sigmas \\ninsert GauassianBlur(images[ i ], sigma) into scale \\nend \\ninsert scale into blurred_images\\nend \\nDOG=Array [] for i in blurred_images \\n\\nscale=Array [] \\nfor index in 1:length(i)-1 \\ninsert (i[index]-i[index+1]) into Scale \\nend \\ninsert scale into DOG \\nkeypoint= Array [] \\nfor each scale_image in DOG \\nfor each image in scale_image \\nfor each pixel in image \\nif image[pixel]>image[8-neighbourhood] and image[pixel] > other images[pixel] in sam \\ninsert pixel into keypoint\\nremove keypoints if near an edge or if keypoint is a low contrast pixel orientation=[] \\nfor keypoint in keypoints: \\na neighbourhood of pixels are selected around the keypoint gradient_magnitude,gradient_direction are found in that region all points above 80% of max maginitude are selected and considered to calculate orientation, \\nadd such points on orientation subtract localised orientation to make it rotation independent.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "'''\n",
        "image_shape=size(image) \n",
        "images= Array [ ] \n",
        "for size in 1:5 \n",
        "resized_image=image.reshape(image_shape//size) \n",
        "insert resized_image into images \n",
        "end\n",
        "blurred_images=[ ] \n",
        "sigmas=Array[] # predecided values of sigma for example from 0.1,1.5,3.0,4.5,6.0 \n",
        "for i in 1:5 \n",
        "scale=Array [] \n",
        "for sigma in sigmas \n",
        "insert GauassianBlur(images[ i ], sigma) into scale \n",
        "end \n",
        "insert scale into blurred_images\n",
        "end \n",
        "DOG=Array [] for i in blurred_images \n",
        "\n",
        "scale=Array [] \n",
        "for index in 1:length(i)-1 \n",
        "insert (i[index]-i[index+1]) into Scale \n",
        "end \n",
        "insert scale into DOG \n",
        "keypoint= Array [] \n",
        "for each scale_image in DOG \n",
        "for each image in scale_image \n",
        "for each pixel in image \n",
        "if image[pixel]>image[8-neighbourhood] and image[pixel] > other images[pixel] in sam \n",
        "insert pixel into keypoint\n",
        "remove keypoints if near an edge or if keypoint is a low contrast pixel orientation=[] \n",
        "for keypoint in keypoints: \n",
        "a neighbourhood of pixels are selected around the keypoint gradient_magnitude,gradient_direction are found in that region all points above 80% of max maginitude are selected and considered to calculate orientation, \n",
        "add such points on orientation subtract localised orientation to make it rotation independent.'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaGG42bQXfrf"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "SIFT is invariant to scale and orientation, making it possible to extract the same features from images taken from different viewpoints of an object, SIFT extracts those features where are scale and rotation invariant and these keypoints can be used as features for object detection , etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRHJ6mprXlTC"
      },
      "source": [
        "2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssW9Y7BEXOlZ"
      },
      "outputs": [],
      "source": [
        "img1 = cv2.imread('/content/monument1.jpeg')\n",
        "img2 = cv2.imread('/content/monument2.jpeg')\n",
        "def doHarris(img):\n",
        "  grayscale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  grayscale = np.float32(grayscale)\n",
        "  dst = cv2.cornerHarris(grayscale, 2, 3, 0.04)\n",
        "  dst = cv2.dilate(dst, None)\n",
        "  img[dst>0.01*dst.max()] = [0,0,255]\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcYuZKGTXtpv"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(doHarris(img1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL6bnCGiXww1"
      },
      "outputs": [],
      "source": [
        "cv2_imshow(doHarris(img2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuMttmpuX6Gm"
      },
      "outputs": [],
      "source": [
        "query_img_bw = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "train_img_bw = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "orb = cv2.ORB_create()\n",
        "queryKeypoints, queryDescriptors = orb.detectAndCompute(query_img_bw, None)\n",
        "trainKeypoints, trainDescriptors = orb.detectAndCompute(train_img_bw, None)\n",
        "matcher = cv2.BFMatcher()\n",
        "matches = matcher.match(queryDescriptors, trainDescriptors)\n",
        "final_img = cv2.drawMatches(img1, queryKeypoints, img2, trainKeypoints, matches[:20], None)\n",
        "final_img = cv2.resize(final_img, (1000, 650))\n",
        "cv2_imshow(final_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBadkIzjX_RB"
      },
      "outputs": [],
      "source": [
        "query_img_bw = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "train_img_bw = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "orb = cv2.ORB_create()\n",
        "queryKeypoints, queryDescriptors = orb.detectAndCompute(query_img_bw, None)\n",
        "trainKeypoints, trainDescriptors = orb.detectAndCompute(train_img_bw, None)\n",
        "matcher = cv2.BFMatcher()\n",
        "matches = matcher.match(queryDescriptors, trainDescriptors)\n",
        "final_img = cv2.drawMatches(img1, queryKeypoints, img2, trainKeypoints, matches[:20], None)\n",
        "final_img = cv2.resize(final_img, (1000, 650))\n",
        "cv2_imshow(final_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhNrb3yTYSuH"
      },
      "source": [
        "Harris corner detection algorithm identifies the internal corners of an image. The corners of an image are basically identified as the regions in which there are variations in large intensity of the gradient in all possible dimensions and directions.\n",
        "Harris corner detection works as:\n",
        "1. Take the grayscale image of the original one\n",
        "2. Apply Sobel operator to find the x and y gradient values for every pixel in the grayscale image\n",
        "3. For each pixel p in the grayscale image, consider a 3x3 window around it and compute the corner strength function.  Call this its Harris value.\n",
        "4. Find all pixels that exceed a certain threshold and are the local maxima within a certain window.\n",
        "5. For each pixel that meets the criteria in 4, compute a feature descriptor\n",
        "\n",
        "\n",
        "ORB use both the FAST keypoint detector and BRIEF descriptor with some added features to improve the performance. FAST is Features from Accelerated Segment Test used to detect features from the provided image.\n",
        "ORB uses BRIEF descriptor but as the BRIEF performs poorly with rotation. ORB is an efficient alternative to SIFT or SURF algorithms used for feature extraction, in computation cost, matching performance and mainly the patents.\n",
        "We have used the following algorithm:\n",
        "1. Take the input image and convert it into grayscale\n",
        "2. Initialize the ORB detector and detect the keypoints in the input image\n",
        "3. Now we have computed the descriptors belonging to both the images\n",
        "4. Then, the keypoints were matched using the Brute Force matcher\n",
        "5. Show the matched images\n",
        "For this question, we have used the image from two different perspectives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCWGtzfLYZES"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7drTlzHdYOiR"
      },
      "outputs": [],
      "source": [
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOvEoqrbYdpf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGXwI3-MYfgv"
      },
      "outputs": [],
      "source": [
        "!cp /content/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqfsim22YiIR"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d jessicali9530/lfw-dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8mEHnfuZ3T-",
        "outputId": "8b3becb8-ab48-4028-cc68-bf70ba143314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/kaggle/api/kaggle_api_extended.py\", line 164, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d ciplab/real-and-fake-face-detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3UeUeJiaAZW"
      },
      "outputs": [],
      "source": [
        "!unzip real-and-fake-face-detection.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFm-gXhraFvE"
      },
      "outputs": [],
      "source": [
        "!unzip lfw-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOCXqSDoaFsj"
      },
      "outputs": [],
      "source": [
        "original = cv2.imread('/content/lfw-deepfunneled/lfw-deepfunneled/Zico/Zico_0002.jpg')\n",
        "cv2_imshow(original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bYGS0fEaFp2"
      },
      "outputs": [],
      "source": [
        "grayscale_image = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt.xml')\n",
        "detected_faces = face_cascade.detectMultiScale(grayscale_image)\n",
        "for (column, row, width, height) in detected_faces:\n",
        "  cv2.rectangle(original, (column, row),(column + width, row + height), (0, 255, 0), 2)\n",
        "plt.imshow(cv2.cvtColor(original, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv5lFIXPaFnK"
      },
      "outputs": [],
      "source": [
        "import dlib\n",
        "img6 = cv2.imread('/content/lfw-deepfunneled/lfw-deepfunneled/Zico/Zico_0002.jpg')\n",
        "gray = cv2.cvtColor(img6, cv2.COLOR_BGR2GRAY)\n",
        "hogFaceDetector = dlib.get_frontal_face_detector()\n",
        "faces = hogFaceDetector(gray, 1)\n",
        "for (i, rect) in enumerate(faces):\n",
        "  x = rect.left()\n",
        "  y = rect.top()\n",
        "  w = rect.right() - x\n",
        "  h = rect.bottom() - y\n",
        "  cv2.rectangle(img6, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "cv2_imshow(img6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5wa-6P2bHZc"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu5Uf-hLaFka"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame()\n",
        "Word=\"The Car Truck is Driven on Road Highway\".split(\" \")\n",
        "A = [2/7, 1/7, 0, 1/7, 1/7,  1/7, 1/7, 0]\n",
        "B = [2/7, 0, 1/7, 1/7, 1/7,  1/7, 0, 1/7]\n",
        "\n",
        "df['Word']=Word\n",
        "df['A'] = A\n",
        "df['B'] = B\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z22VtM_aFhi"
      },
      "outputs": [],
      "source": [
        "idf = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8j6nrICbMTi"
      },
      "outputs": [],
      "source": [
        "for index, row in df.iterrows():\n",
        "    val = 0\n",
        "    if row['A']>0:\n",
        "        val+=1\n",
        "    if row['B']>0:\n",
        "        val+=1\n",
        "    idf.append(np.log(2/val))\n",
        "df['idf']=idf\n",
        "df['tfidf_A'] = df['A']*df['idf']\n",
        "df['tfidf_B'] = df['B']*df['idf']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYyMRG1mbN7h"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9JcTaovbPlR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.bar(df['Word'],df['tfidf_A'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAS5ExoubRXQ"
      },
      "outputs": [],
      "source": [
        "plt.bar(df['Word'],df['tfidf_B'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny7xlwqkbn-J"
      },
      "outputs": [],
      "source": [
        "dataset = ['The car is driven on the road','The truck is driven on the highway']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB6N4eYnbS53"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
        "tfIdf = tfIdfVectorizer.fit_transform(dataset)\n",
        "df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
        "print (df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAtLb5oqbUvv"
      },
      "outputs": [],
      "source": [
        "S1 = 'The car is driven on the road'\n",
        "S2 = 'The truck is driven on the highway'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWlWgff4bWe3"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(use_idf=True)\n",
        "response = vectorizer.fit_transform([S1,S2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udlnQcZkbYWm"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCZgdOv6cz9r"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpOwPdy0cP2r",
        "outputId": "e8a98f9c-3ce5-4e54-af9e-64431c76dbe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW9TLLAzbmUr"
      },
      "outputs": [],
      "source": [
        "feature_params = dict(maxCorners = 300, qualityLevel = 0.2, minDistance = 2, blockSize = 7)\n",
        "lk_params = dict(winSize = (15, 15), maxLevel = 2, criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/video.mp4\")\n",
        "color = (0,255,0)\n",
        "ret, first_frame = cap.read()\n",
        "prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
        "prev = cv2.goodFeaturesToTrack(prev_gray, mask = None, **feature_params)\n",
        "mask = np.zeros_like(first_frame)\n",
        "while (cap.isOpened()):\n",
        "  ret, frame = cap.read()\n",
        "  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "  prev = cv2.goodFeaturesToTrack(prev_gray, mask=None, **feature_params)\n",
        "  next, status, error = cv2.calcOpticalFlowPyrLK(prev_gray, gray, prev, None, **lk_params)\n",
        "  good_old = prev[status == 1].astype(int)\n",
        "  good_new = next[status == 1].astype(int)\n",
        "  for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
        "    a, b = new.ravel()\n",
        "    c, d = old.ravel()\n",
        "    mask = cv2.line(mask, (a, b), (c, d), color, 2)\n",
        "    frame = cv2.circle(frame, (a, b), 3, color, -1)\n",
        "  output = cv2.add(frame, mask)\n",
        "  prev_gray = gray.copy()\n",
        "  prev = good_new.reshape(-1 , 2)\n",
        "  cv2_imshow(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy1CV-AQb_V7"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/video.mp4\")\n",
        "ret, first_frame = cap.read()\n",
        "prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
        "mask = np.zeros_like(first_frame)\n",
        "mask[...,1] = 255\n",
        "while(cap.isOpened()):\n",
        "  ret, frame = cap.read()\n",
        "  cv2_imshow(frame)\n",
        "  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "  flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "  magnitude, angle = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
        "  mask[...,0] = angle * 180 / np.pi / 2\n",
        "  mask[...,2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "  rgb = cv2. cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
        "  cv2_imshow(rgb)\n",
        "  prev_gray = gray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8iim8y7c94w"
      },
      "source": [
        "## Question 6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import feature\n",
        "import glob\n",
        "images = []\n",
        "labels = []\n",
        "path = \"/content/real_and_fake_face/training_real/*.*\"\n",
        "for file in glob.glob(path):\n",
        "  img6 = cv2.imread(file)\n",
        "  img6 = cv2.resize(img6, (128, 256))\n",
        "  hog_desc = feature.hog(img6, orientations=9, pixels_per_cell=(8, 8),\n",
        "                         cells_per_block=(2, 2), \n",
        "                         transform_sqrt=True, block_norm='L2-Hys')\n",
        "  images.append(hog_desc)\n",
        "  labels.append(1)\n",
        "# print(labels)"
      ],
      "metadata": {
        "id": "HsLYkTy0zwnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/real_and_fake_face/training_fake/*.*\"\n",
        "for file in glob.glob(path):\n",
        "  img = cv2.imread(file)\n",
        "  img = cv2.resize(img, (128, 256))\n",
        "  hog_desc = feature.hog(img, orientations=9, pixels_per_cell=(8, 8), \n",
        "                        cells_per_block=(2,2), transform_sqrt=True,\n",
        "                         block_norm='L2-Hys')\n",
        "  images.append(hog_desc)\n",
        "  labels.append(0)"
      ],
      "metadata": {
        "id": "iwAeBBWtzwns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(images))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2cecc15-f63d-488f-a9f1-7b47277f92c8",
        "id": "35dT07N2zwnt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "u_aO_-n6zwnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "svm_model = LinearSVC(random_state = 42, tol=1e-5)\n",
        "clf = svm_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "o0lHsH4fzwnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "predictions = svm_model.predict(X_test)\n",
        "print(accuracy_score(y_test, predictions)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4d7aa5-a5e8-4cdd-bb3d-83ef3c489fee",
        "id": "mMHOcHD-zwnu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54.01174168297456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmGhk1GdIWoc"
      },
      "source": [
        "##Question 7 \n",
        "part one "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTsv84wXUi8e"
      },
      "source": [
        "In this approach, we have applied a function ‘hog_detect’ for evaluating the hog descriptions of each image of the handwritten digits datasets and saved the output of each descriptions (which is an array) into an array which works as the feature set for the SVM model. We have applied SVC and applied polynomial kernel as the datasets are divided into multiple labels rather than being binary labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dcjNhvRvzob"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, roc_curve\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqOBgtPKv1R7"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWW5yQr6Ua5O"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1)).astype('float32')\n",
        "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1)).astype('float32')\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "# one hot encode outputs\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]\n",
        "# define the larger model\n",
        "def larger_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D())\n",
        "\tmodel.add(Conv2D(15, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D())\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu'))\n",
        "\tmodel.add(Dense(50, activation='relu'))\n",
        "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "# build the model\n",
        "model = larger_model()\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram of Oriented Gradients (HOG) is a feature descriptor that helps us finding the explicit and robust local features of an image. A feature descriptor is a representation of an image or an image patch that is a simplified version obtained by extracting useful information and throwing away extraneous information from the original image. In the HOG feature descriptor, the distribution (histograms) of directions of gradients (oriented gradients) are used as features. Gradients (x and y derivatives) of an image are useful because the magnitude of gradients is large around edges/corners which contain abrupt changes in intensity which contains lot of information regarding the object shape as compared to flatter regions. In HOG,feature extraction is performed by first dividng the complete image into smaller regions and calculating the gradients and orientation for each of these regions. This is followed by normalization over blocks of cells to reduce lighting variation since gradients of image are sensitive to overall lighting."
      ],
      "metadata": {
        "id": "XUFCU-kPvlbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "part two"
      ],
      "metadata": {
        "id": "-UQ2LgI_wIx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from matplotlib import pyplot \n",
        "from skimage.transform import resize\n",
        "import math\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "import pickle\n",
        "#loading\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "\n",
        "train_X = np.array(~train_X[:50000] , dtype=float)\n",
        "train_y = np.array(train_y[:50000])\n",
        "test_img = np.array(~test_X[:10000] , dtype=float)\n",
        "test_label = np.array(test_y[:10000])\n",
        "\n",
        "training_img = np.pad(train_X, ((0,0), (2,2), (2, 2)), 'constant')\n",
        "test_img = np.pad(test_img, ((0,0), (2,2), (2, 2)), 'constant')\n",
        "\n",
        "training_gradiant_x = np.pad(training_img, ((0,0), (1,1), (1, 1)), 'constant')\n",
        "training_gradiant_y = np.pad(training_img, ((0,0), (1,1), (1, 1)), 'constant')\n",
        "\n",
        "test_gradiant_x = np.pad(test_img, ((0,0), (1,1), (1, 1)), 'constant')\n",
        "test_gradiant_y = np.pad(test_img, ((0,0), (1,1), (1, 1)), 'constant')\n",
        "\n",
        "training_Magnitude = np.zeros((50000,32,32))\n",
        "training_Angle = np.zeros((50000,32,32))\n",
        "\n",
        "test_Magnitude = np.zeros((50000,32,32))\n",
        "test_Angle = np.zeros((50000,32,32))\n",
        "\n",
        "training_feature_vector = np.zeros((50000,9,4,9))\n",
        "test_feature_vector = np.zeros((10000,9,4,9))\n",
        "\n",
        "def calc_Gradiant_x(x,images_array):\n",
        "    \n",
        "    for i in range(x):\n",
        "        for j in range(1,34):\n",
        "            for k in range(1,33):\n",
        "                images_array[i][j][k] = images_array[i][j][k+1] - images_array[i][j][k-1] / 2\n",
        "    \n",
        "    return images_array\n",
        "\n",
        "def calc_Gradiant_y(x,images_array):\n",
        "    \n",
        "    for i in range(x):\n",
        "        for j in range(1,33):\n",
        "            for k in range(1,34):\n",
        "                images_array[i][j][k] = images_array[i][j+1][k] - images_array[i][j-1][k]  / 2\n",
        "    \n",
        "    return images_array\n",
        "\n",
        "def calc_Magnitude(x,gradiant_x,gradiant_y):\n",
        "    images_array = np.zeros((x,32,32))\n",
        "    for i in range(x):\n",
        "        for j in range(0,32):\n",
        "            for k in range(0,32):\n",
        "                images_array[i][j][k] = math.sqrt( pow(gradiant_x[i][j+1][k+1],2) + pow(gradiant_y[i][j+1][k+1],2) )\n",
        "    return images_array   \n",
        "\n",
        "def calc_Angle(x,gradiant_x,gradiant_y):\n",
        "    images_array = np.zeros((x,32,32))\n",
        "    for i in range(x):\n",
        "        for j in range(0,32):\n",
        "            for k in range(0,32):\n",
        "                images_array[i][j][k] = np.rad2deg(np.arctan(gradiant_y[i][j+1][k+1] / (gradiant_x[i][j+1][k+1]+0.0000001)))\n",
        "                images_array[i][j][k] = math.ceil(images_array[i][j][k]%180)\n",
        "    return images_array            \n",
        "\n",
        "def calc_feature_vector(x,training_Magnitude,training_Angle):\n",
        "    images_array =np.zeros((x,9,4,9))\n",
        "    \n",
        "    \n",
        "    for pic in range(x):\n",
        "        c1 = 0  \n",
        "        c2 = 0\n",
        "        for block in range(9):\n",
        "            if block == 1 :\n",
        "                c1=0\n",
        "                c2=8\n",
        "            elif block == 2:\n",
        "                c1=0\n",
        "                c2=16\n",
        "            elif block == 3:\n",
        "                c1 =8\n",
        "                c2 = 0\n",
        "            elif block == 4: \n",
        "                c1=8\n",
        "                c2=8\n",
        "            elif block == 5:\n",
        "                c1=8\n",
        "                c2=16\n",
        "            elif block == 6:\n",
        "                c1=16 \n",
        "                c2=0 \n",
        "            elif block == 7:\n",
        "                c1=16\n",
        "                c2=8\n",
        "            elif block == 8:\n",
        "                c1=16\n",
        "                c2=16\n",
        "            for cell in range(4):\n",
        "                \n",
        "                if cell == 1 :\n",
        "                    c2+=8\n",
        "                elif cell == 2:\n",
        "                    c1+=8\n",
        "                    c2-=8\n",
        "                elif cell == 3:\n",
        "                    c2+=8\n",
        "                for i in range(c1,c1+8):\n",
        "                    for j in range(c2,c2+8):\n",
        "                        print(str(pic))\n",
        "                        x = training_Angle[pic][i][j]\n",
        "                        if x==180:\n",
        "                            images_array[pic][block][cell][8] = training_Magnitude[pic][i][j]\n",
        "                            continue\n",
        "                        p1 = (x - math.ceil(x/20)*20 - 10) / ((math.floor(x/20)+1)*20)\n",
        "                        p2 = (x - math.ceil(x/20)*20 + 10) / ((math.floor(x/20)+1)*20)\n",
        "                        \n",
        "                        if p1 <= 0 :\n",
        "                            images_array[pic][block][cell][math.floor(x/20)] += training_Magnitude[pic][i][j]\n",
        "                            \n",
        "                        else :\n",
        "                            images_array[pic][block][cell][math.floor(x/20)] += p2 * training_Magnitude[pic][i][j]\n",
        "                            images_array[pic][block][cell][math.floor(x/20) + 1] += p1 * training_Magnitude[pic][i][j]\n",
        "                  \n",
        "    return images_array \n",
        "\n",
        "def normalization(x,images_array):\n",
        "    temp = []\n",
        "    for pic in range(x):\n",
        "        for block in range(9):\n",
        "            Sum = np.sum(images_array[pic][block]) \n",
        "            if Sum == 0:\n",
        "                continue\n",
        "            for i in range(4):\n",
        "                for j in range(9):\n",
        "                    images_array[pic][block][i][j] = images_array[pic][block][i][j] / Sum\n",
        "        temp.append(images_array[pic].flatten().tolist())         \n",
        "    \n",
        "    return temp     \n",
        "\n",
        "def training(training_img):      \n",
        "    calc_Gradiant_x(10000,training_gradiant_x)\n",
        "    calc_Gradiant_y(10000,training_gradiant_y)\n",
        "    training_Magnitude = calc_Magnitude(10000,training_gradiant_x,training_gradiant_y)\n",
        "    training_Angle = calc_Angle(10000,training_gradiant_x,training_gradiant_y)\n",
        "    training_feature_vector = calc_feature_vector(10000,training_Magnitude,training_Angle)\n",
        "    training_fv = normalization(10000,training_feature_vector)\n",
        "\n",
        "    np.save('training_fv.txt',training_fv)\n",
        "    return training_fv\n",
        "\n",
        "def test(test_img):      \n",
        "    calc_Gradiant_x(3000,test_gradiant_x)\n",
        "    calc_Gradiant_y(3000,test_gradiant_y)\n",
        "    test_Magnitude = calc_Magnitude(3000,test_gradiant_x,test_gradiant_y)\n",
        "    test_Angle = calc_Angle(3000,test_gradiant_x,test_gradiant_y)\n",
        "    test_feature_vector = calc_feature_vector(3000,test_Magnitude,test_Angle)\n",
        "    test_fv = normalization(3000,test_feature_vector)\n",
        "    np.save('test_fv.txt',test_fv)\n",
        "    return test_fv\n",
        "\n",
        "def fo(training_fv,train_y,test_fv,test_label):\n",
        "    training_fv = np.asarray(training_fv , dtype='float')\n",
        "    test_fv = np.asarray(test_fv ,dtype = 'float')\n",
        "    \n",
        "\n",
        "    clf = svm.SVC()\n",
        "    clf.fit(training_fv,train_y)\n",
        "    \n",
        "    pred = clf.predict(test_fv)\n",
        "\n",
        "    filename = 'SvMClass.sav'\n",
        "    pickle.dump(clf, open(filename, 'wb'))\n",
        "\n",
        "    print(\"Accurate percentage: \" + str(accuracy_score(test_label, pred)))\n",
        "    print('\\n')\n",
        "    print(classification_report(test_label, pred))\n",
        "    \n",
        "\n",
        "def main():\n",
        "    training_fv = training(training_img)\n",
        "    print(\"train\")\n",
        "    test_fv = test(test_img)\n",
        "    print(\"test\")\n",
        "    fo(training_fv,train_y[:10000],test_fv,test_label[:3000])\n",
        "    \n",
        "main()"
      ],
      "metadata": {
        "id": "zn0oidaZv_po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result = pd.DataFrame(y_pred, columns=['Label'])\n",
        "df_result.insert(0, 'ImageId', range(1, 1 + len(y_pred)))\n",
        "df_result.to_csv('result.csv', index=False)\n",
        "df_result.head()"
      ],
      "metadata": {
        "id": "X96seF-fxDrk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}